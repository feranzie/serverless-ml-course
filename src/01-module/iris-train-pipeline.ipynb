{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2kLrOh-bpGy"
   },
   "source": [
    "# Iris Flower Train and Publish Model\n",
    "\n",
    "\n",
    "In this notebook we will, \n",
    "\n",
    "1. Load the Iris Flower dataset into random split (train/test) DataFrames using a Feature View\n",
    "2. Train a KNN Model using SkLearn\n",
    "3. Evaluate model performance on the test set\n",
    "4. Register the model with Hopsworks Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks\n",
      "  Using cached hopsworks-3.0.5.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hsfs[python]<3.1.0,>=3.0.0\n",
      "  Using cached hsfs-3.0.5.tar.gz (120 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hsml<3.1.0,>=3.0.0\n",
      "  Using cached hsml-3.0.3.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyhumps==1.6.1\n",
      "  Using cached pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from hopsworks) (2.19.1)\n",
      "Collecting furl\n",
      "  Using cached furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.26.32-py3-none-any.whl (132 kB)\n",
      "Collecting pyjks\n",
      "  Using cached pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting mock\n",
      "  Using cached mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from hopsworks) (4.26.0)\n",
      "Collecting hsfs[python]<3.1.0,>=3.0.0\n",
      "  Using cached hsfs-3.0.4.tar.gz (120 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hsfs-3.0.3.tar.gz (120 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hsfs-3.0.2.tar.gz (119 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hsfs-3.0.1.tar.gz (119 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hsfs-3.0.0.tar.gz (119 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of pyhumps to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of hopsworks to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hopsworks\n",
      "  Using cached hopsworks-3.0.4.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hopsworks-3.0.3.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hopsworks-3.0.2.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached hopsworks-3.0.1.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "\n",
      "The conflict is caused by:\n",
      "    hsfs[python] 3.0.5 depends on pandas>=1.2.0\n",
      "    hsfs[python] 3.0.4 depends on pandas>=1.2.0\n",
      "    hsfs[python] 3.0.3 depends on pandas>=1.2.0\n",
      "    hsfs[python] 3.0.2 depends on pandas>=1.2.0\n",
      "    hsfs[python] 3.0.1 depends on pandas>=1.2.0\n",
      "    hsfs[python] 3.0.0 depends on pandas>=1.2.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install hsfs[python]==3.0.0, hsfs[python]==3.0.1, hsfs[python]==3.0.2, hsfs[python]==3.0.3, hsfs[python]==3.0.4 and hsfs[python]==3.0.5 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip install -U hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRtpj-psbpG8"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import hopsworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get a feature_view for the iris flower dataset, or create one if it does not already exist.\n",
    "If you are running this notebook for the first time, it will create the feature view, which contains all of the columns from the **iris feature group**.\n",
    "\n",
    "There are 5 columns: 4 of them are \"features\", and the **variety** column is the **label** (what we are trying to predict using the 4 feature values in the label's row). The label is often called the **target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nRmFM7vcbpHA",
    "outputId": "d920d168-9818-40c5-c292-4cf0afcbbcfd"
   },
   "outputs": [],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "try: \n",
    "    feature_view = fs.get_feature_view(name=\"iris\", version=1)\n",
    "except:\n",
    "    iris_fg = fs.get_feature_group(name=\"iris\", version=1)\n",
    "    query = iris_fg.select_all()\n",
    "    feature_view = fs.create_feature_view(name=\"iris\",\n",
    "                                      version=1,\n",
    "                                      description=\"Read from Iris flower dataset\",\n",
    "                                      labels=[\"variety\"],\n",
    "                                      query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read our features and labels split into a **train_set** and a **test_set**. You split your data into a train_set and a test_set, because you want to train your model on only the train_set, and then evaluate its performance on data that was not seen during training, the test_set. This technique helps evaluate the ability of your model to accurately predict on data it has not seen before.\n",
    "\n",
    "We can ask the feature_view to return a **train_test_split** and it returns:\n",
    "\n",
    "* **X_** is a vector of features, so **X_train** is a vector of features from the **train_set**. \n",
    "* **y_** is a scale of labels, so **y_train** is a scalar of labels from the **train_set**. \n",
    "\n",
    "Note: a vector is an array of values and a scalar is a single value.\n",
    "\n",
    "Note: that mathematical convention is that a vector is denoted by an uppercase letter (hence \"X\") and a scalar is denoted by a lowercase letter (hence \"y\").\n",
    "\n",
    "**X_test** is the features and **y_test** is the labels from our holdout **test_set**. The **test_set** is used to evaluate model performance after the model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR8HeEs6bpHB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit a model to our features and labels from our training set (**X_train** and **y_train**). \n",
    "\n",
    "Fitting a model to a dataset is more commonly called \"training a model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNZcUPHJPIu9",
    "outputId": "389acb4d-74ff-46f1-dee8-a7c27ee79a09"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=2)\n",
    "model.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have trained our model. We can evaluate our model on the **test_set** to estimate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHuAD3ttP8Ep"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can report on how accurate these predictions (**y_pred**) are compared to the labels (the actual results - **y_test**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8EC4_SvbpHE",
    "outputId": "5d73b375-76f0-4518-8e88-4db23e8f2486"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "results = confusion_matrix(y_test, y_pred)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the confusion matrix results that we have 1 or 2 incorrect predictions.\n",
    "We have only 30 flowers in our test set - **y_test**.\n",
    "Our model predicted 1 or 2 flowers were of type \"Virginica\", but the flowers were, in fact, \"Versicolor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "df_cm = pd.DataFrame(results, ['True Setosa', 'True Versicolor', 'True Virginica'],\n",
    "                     ['Pred Setosa', 'Pred Versicolor', 'Pred Virginica'])\n",
    "\n",
    "cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "fig = cm.get_figure()\n",
    "fig.savefig(\"assets/confusion_matrix.png\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the Model with Hopsworks Model Registry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "import os\n",
    "import joblib\n",
    "import hopsworks\n",
    "import shutil\n",
    "\n",
    "project =  hopsworks.login()\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# The 'iris_model' directory will be saved to the model registry\n",
    "model_dir=\"iris_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "joblib.dump(model, model_dir + \"/iris_model.pkl\")\n",
    "shutil.copyfile(\"assets/confusion_matrix.png\", model_dir + \"/confusion_matrix.png\")\n",
    "\n",
    "input_example = X_train.sample()\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema, output_schema)\n",
    "\n",
    "iris_model = mr.python.create_model(\n",
    "    version=1,\n",
    "    name=\"iris\", \n",
    "    metrics={\"accuracy\" : metrics['accuracy']},\n",
    "    model_schema=model_schema,\n",
    "    input_example=input_example, \n",
    "    description=\"Iris Flower Predictor\")\n",
    "\n",
    "iris_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
